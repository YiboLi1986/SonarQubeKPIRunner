import os
import sys 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from langchain_community.chat_models import ChatOllama

class LLMCoderHandler:
    def __init__(self, model_name: str = "llama3.1:8b-instruct-q4_K_M", base_url: str = "http://172.22.5.186:32000/ollama-dev", temperature: float = 0.0) -> None:
        """
        Initialize LLMCoderHandler.

        Args:
            model_name (str): The name of the model to be used.
            base_url (str): The base URL of the model service.
            temperature (float): The temperature parameter for response generation.
        """
        self.model_name = model_name
        self.base_url = base_url
        self.temperature = temperature
        
        try:
            self.model = ChatOllama(model=self.model_name, base_url=self.base_url, temperature=self.temperature, num_ctx=32768)
        except Exception as e:
            raise ConnectionError(f"Failed to initialize model '{self.model_name}' at {self.base_url}: {e}")

    def create_user_prompt(self, user_prompt_template: str, code_snippet: str) -> str:
        """
        Creates a user prompt by populating a template with the user's query and a code_snippet.

        Args:
            user_prompt_template (str): The template string for the user prompt, containing placeholders for dynamic content such as the user query and code_snippet.
            code_snippet (str): The code_snippet content to be included in the prompt.

        Returns:
            str: A formatted user prompt with placeholders replaced by the provided values.
        """
        return user_prompt_template.format(code_snippet=code_snippet)

    def handle_chat(self, system_prompt: str, user_prompt: str, max_tokens: int = 32768) -> str:
        """
        Handles the chat interaction by sending the system prompt and user prompt to the model.

        Args:
            system_prompt (str): The system-generated prompt to guide the model's behavior.
            user_prompt (str): The user's input or query.
            max_tokens (int): The maximum number of tokens to generate in the response.

        Returns:
            str: The response content generated by the model.
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # print(f"Sending messages to model: {messages}")

        try:
            # Create a fresh ChatOllama instance every time
            self.model = ChatOllama(model=self.model_name, base_url=self.base_url, temperature=self.temperature)
            response = self.model.invoke(messages, options={"max_tokens": max_tokens})
        except Exception as e:
            raise RuntimeError(f"Error occurred during model invocation: {e}")

        if response and hasattr(response, 'content'):
            return response.content
        else:
            raise ValueError("Received invalid response from the model.")